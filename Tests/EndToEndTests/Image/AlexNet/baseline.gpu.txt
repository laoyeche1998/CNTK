CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz
    Hardware threads: 12
    Total Memory: 57700428 kB
-------------------------------------------------------------------
Copying test data to local directory
=== Running /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet/AlexNetCommon.cntk currentDirectory=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/TestData RunDir=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu DataDir=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/TestData ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet OutputDir=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu DeviceId=0 timestamping=true configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet/AlexNet.cntk
CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 11:36:57

/home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet/AlexNetCommon.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet  OutputDir=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu  DeviceId=0  timestamping=true  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet/AlexNet.cntk
Changed current directory to /tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/TestData
12/09/2017 11:36:57: -------------------------------------------------------------------
12/09/2017 11:36:57: Build info: 

12/09/2017 11:36:57: 		Built time: Dec  8 2017 01:46:20
12/09/2017 11:36:57: 		Last modified date: Wed Nov 15 09:27:10 2017
12/09/2017 11:36:57: 		Build type: release
12/09/2017 11:36:57: 		Build target: GPU
12/09/2017 11:36:57: 		With 1bit-SGD: yes
12/09/2017 11:36:57: 		With ASGD: yes
12/09/2017 11:36:57: 		Math lib: mkl
12/09/2017 11:36:57: 		CUDA version: 9.0.0
12/09/2017 11:36:57: 		CUDNN version: 7.0.4
12/09/2017 11:36:57: 		Build Branch: HEAD
12/09/2017 11:36:57: 		Build SHA1: b130d7735044ce6697bfb963af91445bee740c73
12/09/2017 11:36:57: 		MPI distribution: Open MPI
12/09/2017 11:36:57: 		MPI version: 1.10.7
12/09/2017 11:36:57: -------------------------------------------------------------------
12/09/2017 11:36:57: -------------------------------------------------------------------
12/09/2017 11:36:57: GPU info:

12/09/2017 11:36:57: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 8112 MB
12/09/2017 11:36:57: -------------------------------------------------------------------

Configuration After Processing and Variable Resolution:

configparameters: AlexNet.cntk:AddTop5Eval=[    
    action=edit
    CurModel=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/models/AlexNet
    NewModel=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/models/AlexNet.Top5
    editPath=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet/add_top5_layer.mel
]

configparameters: AlexNet.cntk:command=Train:AddTop5Eval:Test
configparameters: AlexNet.cntk:ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet
configparameters: AlexNet.cntk:currentDirectory=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/TestData
configparameters: AlexNet.cntk:DataDir=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/TestData
configparameters: AlexNet.cntk:deviceId=0
configparameters: AlexNet.cntk:ModelDir=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/models
configparameters: AlexNet.cntk:ndlMacros=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet/Macros.ndl
configparameters: AlexNet.cntk:numMBsToShowResult=100
configparameters: AlexNet.cntk:OutputDir=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu
configparameters: AlexNet.cntk:parallelTrain=false
configparameters: AlexNet.cntk:precision=float
configparameters: AlexNet.cntk:RunDir=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu
configparameters: AlexNet.cntk:Test=[
    action=test
    modelPath=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/models/AlexNet.Top5
    minibatchSize=4
     NDLNetworkBuilder=[
        networkDescription=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet/AlexNet.ndl
    ]
] [    
    reader=[
        readerType=ImageReader
        file=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet/val_map.txt
        randomize=None
        features=[
            width=227
            height=227
            channels=3
            cropType=Center
        ]
        labels=[
            labelDim=1000
        ]
    ]    
]

configparameters: AlexNet.cntk:timestamping=true
configparameters: AlexNet.cntk:traceLevel=1
configparameters: AlexNet.cntk:Train=[
    action=train
    modelPath=/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/models/AlexNet
    NDLNetworkBuilder=[
        networkDescription=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet/AlexNet.ndl
    ]
    SGD=[
        epochSize=0
        minibatchSize=4
        learningRatesPerMB=0.01*20:0.003*12:0.001*28:0.0003
        momentumPerMB=0.9
        maxEpochs=2
        gradUpdateType=None
        L2RegWeight=0.0005
        dropoutRate=0*5:0.5
        ParallelTrain=[
            parallelizationMethod=DataParallelSGD
            distributedMBReading=true
            parallelizationStartEpoch=1
            DataParallelSGD=[
                gradientBits=1
            ]
        ]
        numMBsToShowResult=100
    ]
] [
    reader=[
        readerType=ImageReader
        file=/home/ubuntu/workspace/Tests/EndToEndTests/Image/AlexNet/train_map.txt
        randomize=Auto
        features=[
            width=227
            height=227
            channels=3
            cropType=RandomSide
            sideRatio=0.875
            jitterType=UniRatio
            interpolations=linear
        ]
        labels=[
            labelDim=1000
        ]
    ]    
]

12/09/2017 11:36:57: Commands: Train AddTop5Eval Test
12/09/2017 11:36:57: precision = "float"

12/09/2017 11:36:57: ##############################################################################
12/09/2017 11:36:57: #                                                                            #
12/09/2017 11:36:57: # Train command (train action)                                               #
12/09/2017 11:36:57: #                                                                            #
12/09/2017 11:36:57: ##############################################################################

parallelTrain option is not enabled. ParallelTrain config will be ignored.
12/09/2017 11:36:57: 
Creating virgin network.
NDLBuilder Using GPU 0
SetGaussianRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
Node 'h2.W' (LearnableParameter operation): Initializating Parameter[4096 x 0] as gaussian later when dimensions are fully known.
Node 'OutputNodes.W' (LearnableParameter operation): Initializating Parameter[1000 x 0] as gaussian later when dimensions are fully known.
Node 'h2.W' (LearnableParameter operation) operation: Tensor shape was inferred as [4096 x 4096].
Node 'OutputNodes.W' (LearnableParameter operation) operation: Tensor shape was inferred as [1000 x 4096].
conv1.c: using cuDNN convolution engine for geometry: Input: 227 x 227 x 3, Output: 57 x 57 x 64, Kernel: 11 x 11 x 3, Map: 1 x 1 x 64, Stride: 4 x 4 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
pool1: using cuDNN convolution engine for geometry: Input: 57 x 57 x 64, Output: 28 x 28 x 64, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv2.c: using cuDNN convolution engine for geometry: Input: 28 x 28 x 64, Output: 28 x 28 x 192, Kernel: 5 x 5 x 64, Map: 1 x 1 x 192, Stride: 1 x 1 x 64, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
pool2: using cuDNN convolution engine for geometry: Input: 28 x 28 x 192, Output: 13 x 13 x 192, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv3.c: using cuDNN convolution engine for geometry: Input: 13 x 13 x 192, Output: 13 x 13 x 384, Kernel: 3 x 3 x 192, Map: 1 x 1 x 384, Stride: 1 x 1 x 192, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
conv4.c: using cuDNN convolution engine for geometry: Input: 13 x 13 x 384, Output: 13 x 13 x 256, Kernel: 3 x 3 x 384, Map: 1 x 1 x 256, Stride: 1 x 1 x 384, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
conv5.c: using cuDNN convolution engine for geometry: Input: 13 x 13 x 256, Output: 13 x 13 x 256, Kernel: 3 x 3 x 256, Map: 1 x 1 x 256, Stride: 1 x 1 x 256, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
pool3: using cuDNN convolution engine for geometry: Input: 13 x 13 x 256, Output: 6 x 6 x 256, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
12/09/2017 11:36:58: 
Model has 48 nodes. Using GPU 0.

12/09/2017 11:36:58: Training criterion:   ce = CrossEntropyWithSoftmax
12/09/2017 11:36:58: Evaluation criterion: err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Gradient Memory Aliasing: 16 are aliased.
	conv5.c (gradient) reuses conv5.z (gradient)
	h1.t (gradient) reuses h1.z (gradient)
	OutputNodes.t (gradient) reuses OutputNodes.z (gradient)
	conv1.c (gradient) reuses conv1.z (gradient)
	conv3.c (gradient) reuses conv3.z (gradient)
	h2.t (gradient) reuses h2.z (gradient)
	conv4.c (gradient) reuses conv4.z (gradient)
	conv2.c (gradient) reuses conv2.z (gradient)

Memory Sharing: Out of 93 matrices, 64 are shared as 12, and 29 are not shared.

Here are the ones that share memory:
	{ conv4.W : [256 x 3456] (gradient)
	  conv4.c : [13 x 13 x 256 x *]
	  conv4.y : [13 x 13 x 256 x *] }
	{ conv3.y : [13 x 13 x 384 x *] (gradient)
	  conv4.y : [13 x 13 x 256 x *] (gradient)
	  conv4.z : [13 x 13 x 256 x *]
	  conv5.c : [13 x 13 x 256 x *]
	  conv5.y : [13 x 13 x 256 x *]
	  pool1 : [28 x 28 x 64 x *] (gradient) }
	{ conv2.W : [192 x 1600] (gradient)
	  conv2.y : [28 x 28 x 192 x *] }
	{ conv1.b : [1 x 1 x 64] (gradient)
	  conv1.c : [57 x 57 x 64 x *]
	  conv1.y : [57 x 57 x 64 x *] (gradient)
	  conv2.c : [28 x 28 x 192 x *]
	  conv2.c : [28 x 28 x 192 x *] (gradient)
	  conv2.z : [28 x 28 x 192 x *] (gradient)
	  conv3.c : [13 x 13 x 384 x *]
	  conv3.y : [13 x 13 x 384 x *] }
	{ conv2.b : [1 x 1 x 192] (gradient)
	  conv5.y : [13 x 13 x 256 x *] (gradient)
	  h1.t : [4096 x *]
	  h1.t : [4096 x *] (gradient)
	  h1.z : [4096 x *] (gradient)
	  h1_d : [4096 x *] (gradient)
	  h2.y : [4096 x *]
	  pool2 : [13 x 13 x 192 x *] (gradient) }
	{ conv5.W : [256 x 2304] (gradient)
	  h1.y : [4096 x *] }
	{ h2.W : [4096 x 4096] (gradient)
	  h2_d : [4096 x *] }
	{ h1.b : [4096] (gradient)
	  h1_d : [4096 x *] }
	{ conv3.W : [384 x 1728] (gradient)
	  pool3 : [6 x 6 x 256 x *] }
	{ OutputNodes.t : [1000 x *]
	  OutputNodes.t : [1000 x *] (gradient)
	  OutputNodes.z : [1000 x *] (gradient)
	  h1.W : [4096 x 6 x 6 x 256] (gradient)
	  h1.z : [4096 x *]
	  h2.t : [4096 x *]
	  h2.y : [4096 x *] (gradient) }
	{ conv1.W : [64 x 363] (gradient)
	  conv1.y : [57 x 57 x 64 x *] }
	{ OutputNodes.z : [1000 x *]
	  conv1.c : [57 x 57 x 64 x *] (gradient)
	  conv1.z : [57 x 57 x 64 x *]
	  conv1.z : [57 x 57 x 64 x *] (gradient)
	  conv2.y : [28 x 28 x 192 x *] (gradient)
	  conv2.z : [28 x 28 x 192 x *]
	  conv3.c : [13 x 13 x 384 x *] (gradient)
	  conv3.z : [13 x 13 x 384 x *]
	  conv3.z : [13 x 13 x 384 x *] (gradient)
	  conv4.c : [13 x 13 x 256 x *] (gradient)
	  conv4.z : [13 x 13 x 256 x *] (gradient)
	  conv5.c : [13 x 13 x 256 x *] (gradient)
	  conv5.z : [13 x 13 x 256 x *]
	  conv5.z : [13 x 13 x 256 x *] (gradient)
	  h1.y : [4096 x *] (gradient)
	  h2.t : [4096 x *] (gradient)
	  h2.z : [4096 x *]
	  h2.z : [4096 x *] (gradient)
	  h2_d : [4096 x *] (gradient)
	  pool3 : [6 x 6 x 256 x *] (gradient) }

Here are the ones that don't share memory:
	{pool1 : [28 x 28 x 64 x *]}
	{conv3.b : [1 x 1 x 384] (gradient)}
	{h2.b : [4096] (gradient)}
	{OutputNodes.b : [1000] (gradient)}
	{conv5.b : [1 x 1 x 256] (gradient)}
	{ce : [1] (gradient)}
	{pool2 : [13 x 13 x 192 x *]}
	{OutputNodes.W : [1000 x 4096] (gradient)}
	{conv4.b : [1 x 1 x 256] (gradient)}
	{ce : [1]}
	{OutputNodes.W : [1000 x 4096]}
	{OutputNodes.b : [1000]}
	{err : [1]}
	{labels : [1000 x *]}
	{conv1.W : [64 x 363]}
	{conv1.b : [1 x 1 x 64]}
	{conv2.W : [192 x 1600]}
	{conv2.b : [1 x 1 x 192]}
	{conv3.W : [384 x 1728]}
	{conv3.b : [1 x 1 x 384]}
	{conv4.W : [256 x 3456]}
	{conv4.b : [1 x 1 x 256]}
	{conv5.W : [256 x 2304]}
	{conv5.b : [1 x 1 x 256]}
	{h1.W : [4096 x 6 x 6 x 256]}
	{h1.b : [4096]}
	{h2.b : [4096]}
	{h2.W : [4096 x 4096]}
	{features : [227 x 227 x 3 x *]}


12/09/2017 11:36:58: Training 61100840 parameters in 16 out of 16 parameter tensors and 45 nodes with gradient:

12/09/2017 11:36:58: 	Node 'OutputNodes.W' (LearnableParameter operation) : [1000 x 4096]
12/09/2017 11:36:58: 	Node 'OutputNodes.b' (LearnableParameter operation) : [1000]
12/09/2017 11:36:58: 	Node 'conv1.W' (LearnableParameter operation) : [64 x 363]
12/09/2017 11:36:58: 	Node 'conv1.b' (LearnableParameter operation) : [1 x 1 x 64]
12/09/2017 11:36:58: 	Node 'conv2.W' (LearnableParameter operation) : [192 x 1600]
12/09/2017 11:36:58: 	Node 'conv2.b' (LearnableParameter operation) : [1 x 1 x 192]
12/09/2017 11:36:58: 	Node 'conv3.W' (LearnableParameter operation) : [384 x 1728]
12/09/2017 11:36:58: 	Node 'conv3.b' (LearnableParameter operation) : [1 x 1 x 384]
12/09/2017 11:36:58: 	Node 'conv4.W' (LearnableParameter operation) : [256 x 3456]
12/09/2017 11:36:58: 	Node 'conv4.b' (LearnableParameter operation) : [1 x 1 x 256]
12/09/2017 11:36:58: 	Node 'conv5.W' (LearnableParameter operation) : [256 x 2304]
12/09/2017 11:36:58: 	Node 'conv5.b' (LearnableParameter operation) : [1 x 1 x 256]
12/09/2017 11:36:58: 	Node 'h1.W' (LearnableParameter operation) : [4096 x 6 x 6 x 256]
12/09/2017 11:36:58: 	Node 'h1.b' (LearnableParameter operation) : [4096]
12/09/2017 11:36:58: 	Node 'h2.W' (LearnableParameter operation) : [4096 x 4096]
12/09/2017 11:36:58: 	Node 'h2.b' (LearnableParameter operation) : [4096]

12/09/2017 11:36:58: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

12/09/2017 11:37:00: Starting Epoch 1: learning rate per sample = 0.002500  effective momentum = 0.900000  momentum as time constant = 38.0 samples

12/09/2017 11:37:00: Starting minibatch loop.
12/09/2017 11:37:04:  Epoch[ 1 of 2]-Minibatch[   1- 100]: ce = 7.60312805 * 400; err = 0.99500000 * 400; time = 4.2705s; samplesPerSecond = 93.7
12/09/2017 11:37:08:  Epoch[ 1 of 2]-Minibatch[ 101- 200]: ce = 7.01225281 * 400; err = 0.99750000 * 400; time = 3.2823s; samplesPerSecond = 121.9
12/09/2017 11:37:09: Finished Epoch[ 1 of 2]: [Training] ce = 7.22621059 * 1024; err = 0.99609375 * 1024; totalSamplesSeen = 1024; learningRatePerSample = 0.0024999999; epochTime=9.40507s
12/09/2017 11:37:12: SGD: Saving checkpoint model '/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/models/AlexNet.1'

12/09/2017 11:37:14: Starting Epoch 2: learning rate per sample = 0.002500  effective momentum = 0.900000  momentum as time constant = 38.0 samples

12/09/2017 11:37:14: Starting minibatch loop.
12/09/2017 11:37:17:  Epoch[ 2 of 2]-Minibatch[   1- 100, 50.00%]: ce = 6.83986938 * 400; err = 0.99750000 * 400; time = 3.2639s; samplesPerSecond = 122.6
12/09/2017 11:37:20:  Epoch[ 2 of 2]-Minibatch[ 101- 200, 100.00%]: ce = 6.88081421 * 400; err = 1.00000000 * 400; time = 3.3694s; samplesPerSecond = 118.7
12/09/2017 11:37:22: Finished Epoch[ 2 of 2]: [Training] ce = 6.86811733 * 1024; err = 0.99902344 * 1024; totalSamplesSeen = 2048; learningRatePerSample = 0.0024999999; epochTime=8.46248s
12/09/2017 11:37:25: SGD: Saving checkpoint model '/tmp/cntk-test-20171209080859.615414/Image_AlexNet@release_gpu/models/AlexNet'

12/09/2017 11:37:27: Action "train" complete.


12/09/2017 11:37:27: ##############################################################################
12/09/2017 11:37:27: #                                                                            #
12/09/2017 11:37:27: # AddTop5Eval command (edit action)                                          #
12/09/2017 11:37:27: #                                                                            #
12/09/2017 11:37:27: ##############################################################################

conv1.c: using GEMM convolution engine for geometry: Input: 227 x 227 x 3, Output: 57 x 57 x 64, Kernel: 11 x 11 x 3, Map: 1 x 1 x 64, Stride: 4 x 4 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
pool1: using GEMM convolution engine for geometry: Input: 57 x 57 x 64, Output: 28 x 28 x 64, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv2.c: using GEMM convolution engine for geometry: Input: 28 x 28 x 64, Output: 28 x 28 x 192, Kernel: 5 x 5 x 64, Map: 1 x 1 x 192, Stride: 1 x 1 x 64, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
pool2: using GEMM convolution engine for geometry: Input: 28 x 28 x 192, Output: 13 x 13 x 192, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv3.c: using GEMM convolution engine for geometry: Input: 13 x 13 x 192, Output: 13 x 13 x 384, Kernel: 3 x 3 x 192, Map: 1 x 1 x 384, Stride: 1 x 1 x 192, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
conv4.c: using GEMM convolution engine for geometry: Input: 13 x 13 x 384, Output: 13 x 13 x 256, Kernel: 3 x 3 x 384, Map: 1 x 1 x 256, Stride: 1 x 1 x 384, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
conv5.c: using GEMM convolution engine for geometry: Input: 13 x 13 x 256, Output: 13 x 13 x 256, Kernel: 3 x 3 x 256, Map: 1 x 1 x 256, Stride: 1 x 1 x 256, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
pool3: using GEMM convolution engine for geometry: Input: 13 x 13 x 256, Output: 6 x 6 x 256, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.

12/09/2017 11:37:32: Action "edit" complete.


12/09/2017 11:37:32: ##############################################################################
12/09/2017 11:37:32: #                                                                            #
12/09/2017 11:37:32: # Test command (test action)                                                 #
12/09/2017 11:37:32: #                                                                            #
12/09/2017 11:37:32: ##############################################################################

NDLBuilder Using GPU 0
Node 'h2.W' (LearnableParameter operation): Initializating Parameter[4096 x 0] as gaussian later when dimensions are fully known.
Node 'OutputNodes.W' (LearnableParameter operation): Initializating Parameter[1000 x 0] as gaussian later when dimensions are fully known.
Node 'h2.W' (LearnableParameter operation) operation: Tensor shape was inferred as [4096 x 4096].
Node 'OutputNodes.W' (LearnableParameter operation) operation: Tensor shape was inferred as [1000 x 4096].
conv1.c: using cuDNN convolution engine for geometry: Input: 227 x 227 x 3, Output: 57 x 57 x 64, Kernel: 11 x 11 x 3, Map: 1 x 1 x 64, Stride: 4 x 4 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
pool1: using cuDNN convolution engine for geometry: Input: 57 x 57 x 64, Output: 28 x 28 x 64, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv2.c: using cuDNN convolution engine for geometry: Input: 28 x 28 x 64, Output: 28 x 28 x 192, Kernel: 5 x 5 x 64, Map: 1 x 1 x 192, Stride: 1 x 1 x 64, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
pool2: using cuDNN convolution engine for geometry: Input: 28 x 28 x 192, Output: 13 x 13 x 192, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv3.c: using cuDNN convolution engine for geometry: Input: 13 x 13 x 192, Output: 13 x 13 x 384, Kernel: 3 x 3 x 192, Map: 1 x 1 x 384, Stride: 1 x 1 x 192, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
conv4.c: using cuDNN convolution engine for geometry: Input: 13 x 13 x 384, Output: 13 x 13 x 256, Kernel: 3 x 3 x 384, Map: 1 x 1 x 256, Stride: 1 x 1 x 384, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
conv5.c: using cuDNN convolution engine for geometry: Input: 13 x 13 x 256, Output: 13 x 13 x 256, Kernel: 3 x 3 x 256, Map: 1 x 1 x 256, Stride: 1 x 1 x 256, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
pool3: using cuDNN convolution engine for geometry: Input: 13 x 13 x 256, Output: 6 x 6 x 256, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
evalNodeNames are not specified, using all the default evalnodes and training criterion nodes.


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 48 matrices, 28 are shared as 3, and 20 are not shared.

Here are the ones that share memory:
	{ OutputNodes.z : [1000 x *2]
	  conv4.z : [13 x 13 x 256 x *2]
	  conv5.c : [13 x 13 x 256 x *2]
	  conv5.y : [13 x 13 x 256 x *2]
	  h1.t : [4096 x *2]
	  h1.y : [4096 x *2]
	  h2.z : [4096 x *2]
	  h2_d : [4096 x *2]
	  pool1 : [28 x 28 x 64 x *2]
	  pool2 : [13 x 13 x 192 x *2] }
	{ conv1.c : [57 x 57 x 64 x *2]
	  conv1.y : [57 x 57 x 64 x *2]
	  conv2.z : [28 x 28 x 192 x *2]
	  conv3.z : [13 x 13 x 384 x *2]
	  conv4.c : [13 x 13 x 256 x *2]
	  h1_d : [4096 x *2] }
	{ OutputNodes.t : [1000 x *2]
	  conv1.z : [57 x 57 x 64 x *2]
	  conv2.c : [28 x 28 x 192 x *2]
	  conv2.y : [28 x 28 x 192 x *2]
	  conv3.c : [13 x 13 x 384 x *2]
	  conv3.y : [13 x 13 x 384 x *2]
	  conv4.y : [13 x 13 x 256 x *2]
	  conv5.z : [13 x 13 x 256 x *2]
	  h1.z : [4096 x *2]
	  h2.t : [4096 x *2]
	  h2.y : [4096 x *2]
	  pool3 : [6 x 6 x 256 x *2] }

Here are the ones that don't share memory:
	{labels : [1000 x *2]}
	{h1.b : [4096]}
	{conv1.W : [64 x 363]}
	{conv5.W : [256 x 2304]}
	{conv5.b : [1 x 1 x 256]}
	{h1.W : [4096 x 6 x 6 x 256]}
	{OutputNodes.b : [1000]}
	{OutputNodes.W : [1000 x 4096]}
	{err : [1]}
	{ce : [1]}
	{features : [227 x 227 x 3 x *2]}
	{conv2.b : [1 x 1 x 192]}
	{conv2.W : [192 x 1600]}
	{h2.W : [4096 x 4096]}
	{h2.b : [4096]}
	{conv4.b : [1 x 1 x 256]}
	{conv3.b : [1 x 1 x 384]}
	{conv3.W : [384 x 1728]}
	{conv1.b : [1 x 1 x 64]}
	{conv4.W : [256 x 3456]}

12/09/2017 11:37:33: Minibatch[1-100]: err = 0.99750000 * 400; ce = 7.32775886 * 400
12/09/2017 11:37:34: Minibatch[101-125]: err = 1.00000000 * 100; ce = 7.35444952 * 100
12/09/2017 11:37:34: Final Results: Minibatch[1-125]: err = 0.99800000 * 500; ce = 7.33309699 * 500; perplexity = 1530.11318485

12/09/2017 11:37:34: Action "test" complete.

12/09/2017 11:37:34: __COMPLETED__