CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz
    Hardware threads: 12
    Total Memory: 57700428 kB
-------------------------------------------------------------------
=== Running /home/ubuntu/workspace/build/1bitsgd/release/bin/cntk configFile=/home/ubuntu/workspace/Tests/EndToEndTests/BatchNormalization/Spatial/CNTK/../02_BatchNormConv.cntk currentDirectory=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/TestData RunDir=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu DataDir=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/TestData ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/BatchNormalization/Spatial/CNTK/.. OutputDir=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu DeviceId=0 timestamping=true batchNormalizationEngine=cntk
CNTK 2.3.1+ (HEAD b130d7, Dec  8 2017 01:52:00) at 2017/12/09 09:25:43

/home/ubuntu/workspace/build/1bitsgd/release/bin/cntk  configFile=/home/ubuntu/workspace/Tests/EndToEndTests/BatchNormalization/Spatial/CNTK/../02_BatchNormConv.cntk  currentDirectory=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/TestData  RunDir=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu  DataDir=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/TestData  ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/BatchNormalization/Spatial/CNTK/..  OutputDir=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu  DeviceId=0  timestamping=true  batchNormalizationEngine=cntk
Changed current directory to /tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/TestData
12/09/2017 09:25:43: -------------------------------------------------------------------
12/09/2017 09:25:43: Build info: 

12/09/2017 09:25:43: 		Built time: Dec  8 2017 01:46:20
12/09/2017 09:25:43: 		Last modified date: Wed Nov 15 09:27:10 2017
12/09/2017 09:25:43: 		Build type: release
12/09/2017 09:25:43: 		Build target: GPU
12/09/2017 09:25:43: 		With 1bit-SGD: yes
12/09/2017 09:25:43: 		With ASGD: yes
12/09/2017 09:25:43: 		Math lib: mkl
12/09/2017 09:25:43: 		CUDA version: 9.0.0
12/09/2017 09:25:43: 		CUDNN version: 7.0.4
12/09/2017 09:25:43: 		Build Branch: HEAD
12/09/2017 09:25:43: 		Build SHA1: b130d7735044ce6697bfb963af91445bee740c73
12/09/2017 09:25:43: 		MPI distribution: Open MPI
12/09/2017 09:25:43: 		MPI version: 1.10.7
12/09/2017 09:25:43: -------------------------------------------------------------------
12/09/2017 09:25:43: -------------------------------------------------------------------
12/09/2017 09:25:43: GPU info:

12/09/2017 09:25:43: 		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8123 MB; free memory = 8112 MB
12/09/2017 09:25:43: -------------------------------------------------------------------

Configuration After Processing and Variable Resolution:

configparameters: 02_BatchNormConv.cntk:batchNormalizationEngine=cntk
configparameters: 02_BatchNormConv.cntk:command=Train:Test
configparameters: 02_BatchNormConv.cntk:ConfigDir=/home/ubuntu/workspace/Tests/EndToEndTests/BatchNormalization/Spatial/CNTK/..
configparameters: 02_BatchNormConv.cntk:currentDirectory=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/TestData
configparameters: 02_BatchNormConv.cntk:DataDir=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/TestData
configparameters: 02_BatchNormConv.cntk:deviceId=0
configparameters: 02_BatchNormConv.cntk:imageLayout=cudnn
configparameters: 02_BatchNormConv.cntk:initOnCPUOnly=true
configparameters: 02_BatchNormConv.cntk:ModelDir=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/Models
configparameters: 02_BatchNormConv.cntk:ndlMacros=/home/ubuntu/workspace/Tests/EndToEndTests/BatchNormalization/Spatial/CNTK/../Macros.ndl
configparameters: 02_BatchNormConv.cntk:numMBsToShowResult=500
configparameters: 02_BatchNormConv.cntk:OutputDir=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu
configparameters: 02_BatchNormConv.cntk:precision=float
configparameters: 02_BatchNormConv.cntk:RootDir=.
configparameters: 02_BatchNormConv.cntk:RunDir=/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu
configparameters: 02_BatchNormConv.cntk:Test=[
    action = "test"
    modelPath = "/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/Models/02_BatchNormConv"
    minibatchSize = 16
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/TestData/Test_cntk_text.txt"
        input = [
            features = [
                dim = 3072
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]    
]

configparameters: 02_BatchNormConv.cntk:timestamping=true
configparameters: 02_BatchNormConv.cntk:traceLevel=1
configparameters: 02_BatchNormConv.cntk:Train=[
    action = "train"
    modelPath = "/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/Models/02_BatchNormConv"
     NDLNetworkBuilder = [
        networkDescription = "/home/ubuntu/workspace/Tests/EndToEndTests/BatchNormalization/Spatial/CNTK/../02_BatchNormConv.ndl"
    ]
    SGD = [
        epochSize = 1024
        minibatchSize = 64
        learningRatesPerMB = 0.03*7:0.01
        momentumPerMB = 0
        maxEpochs = 2
        L2RegWeight = 0
        dropoutRate = 0
    ]
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/TestData/Train_cntk_text.txt"
        input = [
            features = [
                dim = 3072
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]    
]

12/09/2017 09:25:43: Commands: Train Test
12/09/2017 09:25:43: precision = "float"

12/09/2017 09:25:43: ##############################################################################
12/09/2017 09:25:43: #                                                                            #
12/09/2017 09:25:43: # Train command (train action)                                               #
12/09/2017 09:25:43: #                                                                            #
12/09/2017 09:25:43: ##############################################################################

12/09/2017 09:25:43: 
Creating virgin network.
NDLBuilder Using GPU 0
SetGaussianRandomValue (GPU): creating curand object with seed 2, sizeof(ElemType)==4
c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 3, Kernel: 5 x 5 x 3, Map: 1 x 1 x 3, Stride: 1 x 1 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool1: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 15 x 15 x 3, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
12/09/2017 09:25:44: 
Model has 25 nodes. Using GPU 0.

12/09/2017 09:25:44: Training criterion:   CE = CrossEntropyWithSoftmax
12/09/2017 09:25:44: Evaluation criterion: Err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Gradient Memory Aliasing: 4 are aliased.
	features (gradient) reuses featScaled (gradient)
	OutputNodes.t (gradient) reuses OutputNodes.z (gradient)

Memory Sharing: Out of 42 matrices, 22 are shared as 6, and 20 are not shared.

Here are the ones that share memory:
	{ W : [3 x 75] (gradient)
	  pool1 : [15 x 15 x 3 x *]
	  y : [32 x 32 x 3 x *] (gradient) }
	{ OutputNodes.t : [10 x 1 x *]
	  c : [32 x 32 x 3 x *] (gradient)
	  conv1 : [32 x 32 x 3 x *] (gradient)
	  h1.t : [64 x *] (gradient)
	  h1.y : [64 x 1 x *] (gradient)
	  h1.z : [64 x 1 x *]
	  y : [32 x 32 x 3 x *] }
	{ conv1 : [32 x 32 x 3 x *]
	  sc : [3 x 1] (gradient) }
	{ b : [3 x 1] (gradient)
	  h1.t : [64 x *]
	  h1.y : [64 x 1 x *]
	  pool1 : [15 x 15 x 3 x *] (gradient) }
	{ OutputNodes.t : [10 x 1 x *] (gradient)
	  OutputNodes.z : [10 x 1 x *] (gradient)
	  h1.b : [64 x 1] (gradient) }
	{ OutputNodes.z : [10 x 1 x *]
	  h1.W : [64 x 15 x 15 x 3] (gradient)
	  h1.z : [64 x 1 x *] (gradient) }

Here are the ones that don't share memory:
	{c : [32 x 32 x 3 x *]}
	{OutputNodes.b : [10] (gradient)}
	{featScaled : [32 x 32 x 3 x *]}
	{CE : [1] (gradient)}
	{OutputNodes.W : [10 x 64] (gradient)}
	{Err : [1]}
	{featOffs : [1 x 1]}
	{labels : [10 x *]}
	{W : [3 x 75]}
	{b : [3 x 1]}
	{sc : [3 x 1]}
	{m : [3 x 1]}
	{v : [3 x 1]}
	{y.run_sample_count : [1]}
	{h1.W : [64 x 15 x 15 x 3]}
	{OutputNodes.W : [10 x 64]}
	{OutputNodes.b : [10]}
	{h1.b : [64 x 1]}
	{CE : [1]}
	{features : [32 x 32 x 3 x *]}


12/09/2017 09:25:44: Training 44145 parameters in 7 out of 7 parameter tensors and 17 nodes with gradient:

12/09/2017 09:25:44: 	Node 'OutputNodes.W' (LearnableParameter operation) : [10 x 64]
12/09/2017 09:25:44: 	Node 'OutputNodes.b' (LearnableParameter operation) : [10]
12/09/2017 09:25:44: 	Node 'W' (LearnableParameter operation) : [3 x 75]
12/09/2017 09:25:44: 	Node 'b' (LearnableParameter operation) : [3 x 1]
12/09/2017 09:25:44: 	Node 'h1.W' (LearnableParameter operation) : [64 x 15 x 15 x 3]
12/09/2017 09:25:44: 	Node 'h1.b' (LearnableParameter operation) : [64 x 1]
12/09/2017 09:25:44: 	Node 'sc' (LearnableParameter operation) : [3 x 1]

12/09/2017 09:25:44: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

12/09/2017 09:25:44: Starting Epoch 1: learning rate per sample = 0.000469  effective momentum = 0.000000  momentum as time constant = 0.0 samples

12/09/2017 09:25:44: Starting minibatch loop.
12/09/2017 09:25:49: Finished Epoch[ 1 of 2]: [Training] CE = 2.27208638 * 1024; Err = 0.85546875 * 1024; totalSamplesSeen = 1024; learningRatePerSample = 0.00046874999; epochTime=4.39344s
12/09/2017 09:25:49: SGD: Saving checkpoint model '/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/Models/02_BatchNormConv.1'

12/09/2017 09:25:49: Starting Epoch 2: learning rate per sample = 0.000469  effective momentum = 0.000000  momentum as time constant = 0.0 samples

12/09/2017 09:25:49: Starting minibatch loop.
12/09/2017 09:25:49: Finished Epoch[ 2 of 2]: [Training] CE = 2.21359444 * 1024; Err = 0.80859375 * 1024; totalSamplesSeen = 2048; learningRatePerSample = 0.00046874999; epochTime=0.0301967s
12/09/2017 09:25:49: SGD: Saving checkpoint model '/tmp/cntk-test-20171209080859.615414/BatchNormalization/Spatial_CNTK@release_gpu/Models/02_BatchNormConv'

12/09/2017 09:25:49: Action "train" complete.


12/09/2017 09:25:49: ##############################################################################
12/09/2017 09:25:49: #                                                                            #
12/09/2017 09:25:49: # Test command (test action)                                                 #
12/09/2017 09:25:49: #                                                                            #
12/09/2017 09:25:49: ##############################################################################


Post-processing network...

3 roots:
	CE = CrossEntropyWithSoftmax()
	Err = ClassificationError()
	OutputNodes.z = Plus()

Validating network. 25 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [10 x *1]
Validating --> OutputNodes.W = LearnableParameter() :  -> [10 x 64]
Validating --> h1.W = LearnableParameter() :  -> [64 x 15 x 15 x 3]
Validating --> W = LearnableParameter() :  -> [3 x 75]
Validating --> features = InputValue() :  -> [32 x 32 x 3 x *1]
Validating --> featOffs = LearnableParameter() :  -> [1 x 1]
Validating --> featScaled = Minus (features, featOffs) : [32 x 32 x 3 x *1], [1 x 1] -> [32 x 32 x 3 x *1]
Validating --> c = Convolution (W, featScaled) : [3 x 75], [32 x 32 x 3 x *1] -> [32 x 32 x 3 x *1]
Validating --> sc = LearnableParameter() :  -> [3 x 1]
Validating --> b = LearnableParameter() :  -> [3 x 1]
Validating --> m = LearnableParameter() :  -> [3 x 1]
Validating --> v = LearnableParameter() :  -> [3 x 1]
Validating --> y.run_sample_count = LearnableParameter() :  -> [1]
Validating --> y = BatchNormalization (c, sc, b, m, v, y.run_sample_count) : [32 x 32 x 3 x *1], [3 x 1], [3 x 1], [3 x 1], [3 x 1], [1] -> [32 x 32 x 3 x *1]
Validating --> conv1 = RectifiedLinear (y) : [32 x 32 x 3 x *1] -> [32 x 32 x 3 x *1]
Validating --> pool1 = MaxPooling (conv1) : [32 x 32 x 3 x *1] -> [15 x 15 x 3 x *1]
Validating --> h1.t = Times (h1.W, pool1) : [64 x 15 x 15 x 3], [15 x 15 x 3 x *1] -> [64 x *1]
Validating --> h1.b = LearnableParameter() :  -> [64 x 1]
Validating --> h1.z = Plus (h1.t, h1.b) : [64 x *1], [64 x 1] -> [64 x 1 x *1]
Validating --> h1.y = RectifiedLinear (h1.z) : [64 x 1 x *1] -> [64 x 1 x *1]
Validating --> OutputNodes.t = Times (OutputNodes.W, h1.y) : [10 x 64], [64 x 1 x *1] -> [10 x 1 x *1]
Validating --> OutputNodes.b = LearnableParameter() :  -> [10]
Validating --> OutputNodes.z = Plus (OutputNodes.t, OutputNodes.b) : [10 x 1 x *1], [10] -> [10 x 1 x *1]
Validating --> CE = CrossEntropyWithSoftmax (labels, OutputNodes.z) : [10 x *1], [10 x 1 x *1] -> [1]
Validating --> Err = ClassificationError (labels, OutputNodes.z) : [10 x *1], [10 x 1 x *1] -> [1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.

c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 3, Kernel: 5 x 5 x 3, Map: 1 x 1 x 3, Stride: 1 x 1 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool1: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 15 x 15 x 3, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.



Post-processing network complete.

evalNodeNames are not specified, using all the default evalnodes and training criterion nodes.


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 25 matrices, 10 are shared as 2, and 15 are not shared.

Here are the ones that share memory:
	{ OutputNodes.z : [10 x 1 x *1]
	  c : [32 x 32 x 3 x *1]
	  conv1 : [32 x 32 x 3 x *1]
	  h1.t : [64 x *1]
	  h1.y : [64 x 1 x *1] }
	{ OutputNodes.t : [10 x 1 x *1]
	  featScaled : [32 x 32 x 3 x *1]
	  h1.z : [64 x 1 x *1]
	  pool1 : [15 x 15 x 3 x *1]
	  y : [32 x 32 x 3 x *1] }

Here are the ones that don't share memory:
	{h1.b : [64 x 1]}
	{b : [3 x 1]}
	{W : [3 x 75]}
	{OutputNodes.W : [10 x 64]}
	{featOffs : [1 x 1]}
	{Err : [1]}
	{CE : [1]}
	{y.run_sample_count : [1]}
	{OutputNodes.b : [10]}
	{features : [32 x 32 x 3 x *1]}
	{v : [3 x 1]}
	{sc : [3 x 1]}
	{h1.W : [64 x 15 x 15 x 3]}
	{m : [3 x 1]}
	{labels : [10 x *1]}

12/09/2017 09:25:50: Minibatch[1-500]: Err = 0.81237500 * 8000; CE = 2.18292270 * 8000
12/09/2017 09:25:50: Minibatch[501-625]: Err = 0.81300000 * 2000; CE = 2.18584555 * 2000
12/09/2017 09:25:50: Final Results: Minibatch[1-625]: Err = 0.81250000 * 10000; CE = 2.18350727 * 10000; perplexity = 8.87738714

12/09/2017 09:25:50: Action "test" complete.

12/09/2017 09:25:50: __COMPLETED__